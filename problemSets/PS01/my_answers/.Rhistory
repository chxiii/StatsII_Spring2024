# - p_value: Probability that the null hypothesis is correct
# create empirical distribution of observed data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# generate test statistic
D <- max(abs(empiricalCDF - pnorm(data)))
# R can not deal with infinite, so I use 1000 instead
n <- 1000
# calculate p-value
p_value <- sqrt(2 * pi) / D * sum(exp(-((2 * (1:n) - 1)^2 * pi^2) / (8 * D^2)))
# return results
return(list(D = D, p_value = p_value))
}
# set seed as 123
set.seed(123)
# generate 1,000 Cauchy random variables
data <- rcauchy(1000, location = 0, scale = 1)
# use function to drive Kolmogorov-Smirnov test
ks_results <- ks_normal(data)
# print the test results
paste("D:", ks_results$D)
paste("p_value:", ks_results$p_value)
# check by ks.test function in R
ks_check <- ks.test(data, "pnorm")
print(ks_check)
#####################
# Problem 2
#####################
set.seed (123)
data <- data.frame(x = runif(200, 1, 10))
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
# Draw a scatter to see the distribution
pdf("q2_plot1.pdf")
q2_scatter <- ggplot(data, aes(x = x, y = y)) +
geom_point(shape=1, size=2.8, color="#0F4C75") + # set point shape and color
theme_bw() + # set the coooooolest style
theme(panel.grid = element_blank())  # no gird
print(q2_scatter)
dev.off()
# Define OLS loss function
# Loss function is used to measure the differences between
# predicted value and the real value.
loss_function <- function(theta, x, y){
# This function is used to define OLS loss function
# Parameters:
# - theta: parameter vector, in this case is the intercept and coefficients.
# - x: independent variables value, in this case is a matrix.
# - y: dependent variable value, in this case is a vector.
# Returns:
# - loss: the differences between predicted values and observed values.
# in this case is the sum of squared residuals (SSR).
predicted <- theta[1] + theta[2] * x
loss <- sum((predicted - y)^2)
return(loss)
}
newton_result <- optim(c(0, 0), # set the default slope and intercept to 0
loss_function, # use the loss function
x = data$x, y = data$y, # set the variables values
method = "BFGS") # define the method as BFGS
print("The intercept is:", newton_result$par[1], "\n")
View(newton_result)
cat("The intercept is:", newton_result$par[1], "\n")
cat("The slope is:", newton_result$par[2], "\n")
# Finally, let's compare with lm methods.
lm_method <- lm(y ~ x, data)
summary(lm_method)
View(lm_method)
#####################
# load libraries
# set wd
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"),  pkgTest)
lapply(c("ggplot2"),  pkgTest)
# set wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
#####################
# Problem 1
#####################
# The function of Kolmogorov-Smirnov test with normal distribution
ks_normal <- function(data) {
# This function is used to test if a distribution follows normal distribution.
# Parameters:
# - data: The empirical distribution
# Returns:
# - D: largest absolute difference between the two distribution
# - p_value: Probability that the null hypothesis is correct
# create empirical distribution of observed data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# generate test statistic
D <- max(abs(empiricalCDF - pnorm(data)))
# R can not deal with infinite, so I use 1000 instead
n <- 1000
# calculate p-value
p_value <- sqrt(2 * pi) / D * sum(exp(-((2 * (1:n) - 1)^2 * pi^2) / (8 * D^2)))
# return results
return(list(D = D, p_value = p_value))
}
# set seed as 123
set.seed(123)
# generate 1,000 Cauchy random variables
data <- rcauchy(1000, location = 0, scale = 1)
# use function to drive Kolmogorov-Smirnov test
ks_results <- ks_normal(data)
# print the test results
paste("D:", ks_results$D)
paste("p_value:", ks_results$p_value)
# check by ks.test function in R
ks_check <- ks.test(data, "pnorm")
print(ks_check)
#####################
# Problem 2
#####################
set.seed (123)
data <- data.frame(x = runif(200, 1, 10))
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
# Draw a scatter to see the distribution
pdf("q2_plot1.pdf")
q2_scatter <- ggplot(data, aes(x = x, y = y)) +
geom_point(shape=1, size=2.8, color="#0F4C75") + # set point shape and color
theme_bw() + # set the coooooolest style
theme(panel.grid = element_blank())  # no gird
print(q2_scatter)
dev.off()
# Define OLS loss function
# Loss function is used to measure the differences between
# predicted value and the real value.
loss_function <- function(theta, x, y){
# This function is used to define OLS loss function
# Parameters:
# - theta: parameter vector, in this case is the intercept and coefficients.
# - x: independent variables value, in this case is a matrix.
# - y: dependent variable value, in this case is a vector.
# Returns:
# - loss: the differences between predicted values and observed values.
# in this case is the sum of squared residuals (SSR).
predicted <- theta[1] + theta[2] * x
loss <- sum((predicted - y)^2)
return(loss)
}
newton_results <- optim(c(0, 0), # set the default slope and intercept to 0
loss_function, # use the loss function
x = data$x, y = data$y, # set the variables values
method = "BFGS") # define the method as BFGS
cat("In Newton-Raphson algorithm, my results are: \n")
cat("The intercept is:", newton_results$par[1], "\n")
cat("The slope is:", newton_results$par[2], "\n")
# Finally, let's compare with lm methods.
lm_results <- lm(y ~ x, data)
summary(lm_results)
cat("In lm methods, my results are: \n")
cat("The intercept is:", lm_results$coefficient[1], "\n")
cat("The slope is:", lm_results$coefficient[2], "\n")
#####################
# load libraries
# set wd
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"),  pkgTest)
lapply(c("ggplot2"),  pkgTest)
# set wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
#####################
# Problem 1
#####################
# The function of Kolmogorov-Smirnov test with normal distribution
ks_normal <- function(data) {
# This function is used to test if a distribution follows normal distribution.
# Parameters:
# - data: The empirical distribution
# Returns:
# - D: largest absolute difference between the two distribution
# - p_value: Probability that the null hypothesis is correct
# create empirical distribution of observed data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# generate test statistic
D <- max(abs(empiricalCDF - pnorm(data)))
# R can not deal with infinite, so I use 1000 instead
n <- 1000
# calculate p-value
p_value <- sqrt(2 * pi) / D * sum(exp(-((2 * (1:n) - 1)^2 * pi^2) / (8 * D^2)))
# return results
return(list(D = D, p_value = p_value))
}
# set seed as 123
set.seed(123)
# generate 1,000 Cauchy random variables
data <- rcauchy(1000, location = 0, scale = 1)
# use function to drive Kolmogorov-Smirnov test
ks_results <- ks_normal(data)
# print the test results
paste("D:", ks_results$D)
paste("p_value:", ks_results$p_value)
# check by ks.test function in R
ks_check <- ks.test(data, "pnorm")
print(ks_check)
#####################
# Problem 2
#####################
set.seed (123)
data <- data.frame(x = runif(200, 1, 10))
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
# Draw a scatter to see the distribution
pdf("q2_plot1.pdf")
q2_scatter <- ggplot(data, aes(x = x, y = y)) +
geom_point(shape=1, size=2.8, color="#0F4C75") + # set point shape and color
theme_bw() + # set the coooooolest style
theme(panel.grid = element_blank())  # no gird
print(q2_scatter)
dev.off()
# Define OLS loss function
# Loss function is used to measure the differences between
# predicted value and the real value.
loss_function <- function(theta, x, y){
# This function is used to define OLS loss function
# Parameters:
# - theta: parameter vector, in this case is the intercept and coefficients.
# - x: independent variables value, in this case is a matrix.
# - y: dependent variable value, in this case is a vector.
# Returns:
# - loss: the differences between predicted values and observed values.
# in this case is the sum of squared residuals (SSR).
predicted <- theta[1] + theta[2] * x
loss <- sum((predicted - y)^2)
return(loss)
}
newton_results <- optim(c(0, 0), # set the default slope and intercept to 0
loss_function, # use the loss function
x = data$x, y = data$y, # set the variables values
method = "BFGS") # define the method as BFGS
cat("In Newton-Raphson algorithm, my results are: \n")
cat("The intercept is:", newton_results$par[1], "\n")
cat("The slope is:", newton_results$par[2], "\n")
# Finally, let's compare with lm methods.
lm_results <- lm(y ~ x, data)
# summary(lm_results)
cat("In lm methods, my results are: \n")
cat("The intercept is:", lm_results$coefficient[1], "\n")
cat("The slope is:", lm_results$coefficient[2], "\n")
View(detachAllPackages)
#####################
# load libraries
# set wd
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"),  pkgTest)
lapply(c("ggplot2"),  pkgTest)
# set wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
#####################
# Problem 1
#####################
# The function of Kolmogorov-Smirnov test with normal distribution
ks_normal <- function(data) {
# This function is used to test if a distribution follows normal distribution.
# Parameters:
# - data: The empirical distribution
# Returns:
# - D: largest absolute difference between the two distribution
# - p_value: Probability that the null hypothesis is correct
# create empirical distribution of observed data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# generate test statistic
D <- max(abs(empiricalCDF - pnorm(data)))
# R can not deal with infinite, so I use 1000 instead
n <- 1000
# calculate p-value
p_value <- sqrt(2 * pi) / D * sum(exp(-((2 * (1:n) - 1)^2 * pi^2) / (8 * D^2)))
# return results
return(list(D = D, p_value = p_value))
}
# set seed as 123
set.seed(123)
# generate 1,000 Cauchy random variables
data <- rcauchy(1000, location = 0, scale = 1)
# use function to drive Kolmogorov-Smirnov test
ks_results <- ks_normal(data)
# print the test results
paste("D:", ks_results$D)
paste("p_value:", ks_results$p_value)
# check by ks.test function in R
ks_check <- ks.test(data, "pnorm")
print(ks_check)
#####################
# Problem 2
#####################
set.seed (123)
data <- data.frame(x = runif(200, 1, 10))
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
# Draw a scatter to see the distribution
pdf("q2_plot1.pdf")
q2_scatter <- ggplot(data, aes(x = x, y = y)) +
geom_point(shape=1, size=2.8, color="#0F4C75") + # set point shape and color
theme_bw() + # set the coooooolest style
theme(panel.grid = element_blank())  # no gird
print(q2_scatter)
dev.off()
# Define OLS loss function
# Loss function is used to measure the differences between
# predicted value and real value.
loss_function <- function(theta, x, y){
# This function is used to define OLS loss function
# Parameters:
# - theta: parameter vector, in this case is the intercept and coefficients.
# - x: independent variables value, in this case is a matrix.
# - y: dependent variable value, in this case is a vector.
# Returns:
# - loss: the differences between predicted values and observed values.
# in this case is the sum of squared residuals (SSR).
predicted <- theta[1] + theta[2] * x
loss <- sum((predicted - y)^2)
return(loss)
}
# define parameters we need in optim function
# optim function can used to minimize or maximize an object function
initial_coef <- c(0, 0) # set the default slope and intercept to 0
method <- "BFGS" # define the method as BFGS
newton_results <- optim(initial_coef,
loss_function, # use the loss function
x = data$x, y = data$y, # set the variables values
method)
cat("In Newton-Raphson algorithm, my results are: \n")
cat("The intercept is:", newton_results$par[1], "\n")
cat("The slope is:", newton_results$par[2], "\n")
# Finally, let's compare with lm methods.
lm_results <- lm(y ~ x, data)
summary(lm_results)
cat("In lm methods, my results are: \n")
cat("The intercept is:", lm_results$coefficient[1], "\n")
cat("The slope is:", lm_results$coefficient[2], "\n")
#####################
# load libraries
# set wd
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"),  pkgTest)
lapply(c("ggplot2"),  pkgTest)
# set wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
#####################
# Problem 1
#####################
# The function of Kolmogorov-Smirnov test with normal distribution
ks_normal <- function(data) {
# This function is used to test if a distribution follows normal distribution.
# Parameters:
# - data: The empirical distribution
# Returns:
# - D: largest absolute difference between the two distribution
# - p_value: Probability that the null hypothesis is correct
# create empirical distribution of observed data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# generate test statistic
D <- max(abs(empiricalCDF - pnorm(data)))
# R can not deal with infinite, so I use 1000 instead
n <- 1000
# calculate p-value
p_value <- sqrt(2 * pi) / D * sum(exp(-((2 * (1:n) - 1)^2 * pi^2) / (8 * D^2)))
# return results
return(list(D = D, p_value = p_value))
}
# set seed as 123
set.seed(123)
# generate 1,000 Cauchy random variables
data <- rcauchy(1000, location = 0, scale = 1)
# use function to drive Kolmogorov-Smirnov test
ks_results <- ks_normal(data)
# print the test results
paste("D:", ks_results$D)
paste("p_value:", ks_results$p_value)
# check by ks.test function in R
ks_check <- ks.test(data, "pnorm")
print(ks_check)
#####################
# Problem 2
#####################
set.seed (123)
data <- data.frame(x = runif(200, 1, 10))
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
# Draw a scatter to see the distribution
pdf("q2_plot1.pdf")
q2_scatter <- ggplot(data, aes(x = x, y = y)) +
geom_point(shape=1, size=2.8, color="#0F4C75") + # set point shape and color
theme_bw() + # set the coooooolest style
theme(panel.grid = element_blank())  # no gird
print(q2_scatter)
dev.off()
# Define OLS loss function
# Loss function is used to measure the differences between
# predicted value and real value.
loss_function <- function(theta, x, y){
# This function is used to define OLS loss function
# Parameters:
# - theta: parameter vector, in this case is the intercept and coefficients.
# - x: independent variables value, in this case is a matrix.
# - y: dependent variable value, in this case is a vector.
# Returns:
# - loss: the differences between predicted values and observed values.
# in this case is the sum of squared residuals (SSR).
predicted <- theta[1] + theta[2] * x
loss <- sum((predicted - y)^2)
return(loss)
}
# define parameters we need in optim function
# optim function can used to minimize or maximize an object function
initial_coef <- c(0, 0) # set the default slope and intercept to 0
method <- "BFGS" # define the method as BFGS
newton_results <- optim(initial_coef,
loss_function, # use the loss function
x = data$x, y = data$y, # set the variables values
method)
cat("In Newton-Raphson algorithm, my results are: \n")
cat("The intercept is:", newton_results$par[1], "\n")
cat("The slope is:", newton_results$par[2], "\n")
# Finally, let's compare with lm methods.
lm_results <- lm(y ~ x, data)
summary(lm_results)
cat("In lm methods, my results are: \n")
cat("The intercept is:", lm_results$coefficient[1], "\n")
cat("The slope is:", lm_results$coefficient[2], "\n")
View(newton_results)
# print Newton-Raphson results
cat("In Newton-Raphson algorithm, my results are: \n")
cat("The intercept is:", newton_results$par[1], "\n")
cat("The slope is:", newton_results$par[2], "\n")
# Finally, let's compare with lm methods.
lm_results <- lm(y ~ x, data)
summary(lm_results)
# print OLS results
cat("In lm methods, my results are: \n")
cat("The intercept is:", lm_results$coefficient[1], "\n")
cat("The slope is:", lm_results$coefficient[2], "\n")
